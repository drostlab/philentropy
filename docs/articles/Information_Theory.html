<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Information Theory • philentropy</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Information Theory">
<meta property="og:description" content="philentropy">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">philentropy</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.8.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Distances.html">Distances</a>
    </li>
    <li>
      <a href="../articles/Information_Theory.html">Information Theory</a>
    </li>
    <li>
      <a href="../articles/Introduction.html">Introduction</a>
    </li>
    <li>
      <a href="../articles/Many_Distances.html">Comparing many probability density functions</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/drostlab/philentropy/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Information Theory</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/drostlab/philentropy/blob/HEAD/vignettes/Information_Theory.Rmd" class="external-link"><code>vignettes/Information_Theory.Rmd</code></a></small>
      <div class="hidden name"><code>Information_Theory.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="information-theory-measures-in-philentropy">Information Theory measures in <code>philentropy</code><a class="anchor" aria-label="anchor" href="#information-theory-measures-in-philentropy"></a>
</h2>
<blockquote>
<p>The laws of probability, so true in general, so fallacious in particular.</p>
<p>- Edward Gibbon</p>
</blockquote>
<p>Information theory and statistics were beautifully fused by <code>Solomon Kullback</code>. This fusion allowed to quantify correlations and similarities between random variables using a more sophisticated toolkit. Modern fields such as machine learning and statistical data science build upon this fusion and the most powerful statistical techniques used today are based on an information theoretic foundation.</p>
<p>The <code>philentropy</code> package aims to follow this tradition and therefore, in addition to a comprehensive catalog of distance measures it also implements the most important information theory measures.</p>
<div class="section level3">
<h3 id="shannons-entropy-hx">Shannon’s Entropy H(X)<a class="anchor" aria-label="anchor" href="#shannons-entropy-hx"></a>
</h3>
<blockquote>
<p><span class="math inline">\(H(X) = -\sum\limits_{i=1}^n P(x_i) * log_b(P(x_i))\)</span></p>
</blockquote>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># define probabilities P(X)</span></span>
<span><span class="va">Prob</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="co"># Compute Shannon's Entropy</span></span>
<span><span class="fu"><a href="../reference/H.html">H</a></span><span class="op">(</span><span class="va">Prob</span><span class="op">)</span></span></code></pre></div>
<pre><code>[1] 3.103643</code></pre>
</div>
<div class="section level3">
<h3 id="shannons-joint-entropy-hxy">Shannon’s Joint-Entropy H(X,Y)<a class="anchor" aria-label="anchor" href="#shannons-joint-entropy-hxy"></a>
</h3>
<blockquote>
<p><span class="math inline">\(H(X,Y) = -\sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b(P(x_i, y_j))\)</span></p>
</blockquote>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># define the joint distribution P(X,Y)</span></span>
<span><span class="va">P_xy</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">100</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">100</span><span class="op">)</span></span>
<span><span class="co"># Compute Shannon's Joint-Entropy</span></span>
<span><span class="fu"><a href="../reference/JE.html">JE</a></span><span class="op">(</span><span class="va">P_xy</span><span class="op">)</span></span></code></pre></div>
<pre><code>[1] 6.372236</code></pre>
</div>
<div class="section level3">
<h3 id="shannons-conditional-entropy-hx-y">Shannon’s Conditional-Entropy H(X | Y)<a class="anchor" aria-label="anchor" href="#shannons-conditional-entropy-hx-y"></a>
</h3>
<blockquote>
<p><span class="math inline">\(H(Y|X) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b( P(x_i) / P(x_i, y_j) )\)</span></p>
</blockquote>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># define the distribution P(X)</span></span>
<span><span class="va">P_x</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="co"># define the distribution P(Y)</span></span>
<span><span class="va">P_y</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Shannon's Joint-Entropy</span></span>
<span><span class="fu"><a href="../reference/CE.html">CE</a></span><span class="op">(</span><span class="va">P_x</span>, <span class="va">P_y</span><span class="op">)</span></span></code></pre></div>
<pre><code>[1] 0</code></pre>
</div>
<div class="section level3">
<h3 id="mutual-information-ixy">Mutual Information I(X,Y)<a class="anchor" aria-label="anchor" href="#mutual-information-ixy"></a>
</h3>
<blockquote>
<p><span class="math inline">\(MI(X,Y) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) * log_b( P(x_i, y_j) / ( P(x_i) * P(y_j) )\)</span></p>
</blockquote>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># define the distribution P(X)</span></span>
<span><span class="va">P_x</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="co"># define the distribution P(Y)</span></span>
<span><span class="va">P_y</span> <span class="op">&lt;-</span> <span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">)</span></span>
<span><span class="co"># define the joint-distribution P(X,Y)</span></span>
<span><span class="va">P_xy</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute Shannon's Joint-Entropy</span></span>
<span><span class="fu"><a href="../reference/MI.html">MI</a></span><span class="op">(</span><span class="va">P_x</span>, <span class="va">P_y</span>, <span class="va">P_xy</span><span class="op">)</span></span></code></pre></div>
<pre><code>[1] 3.311973</code></pre>
</div>
<div class="section level3">
<h3 id="kullback-leibler-divergence">Kullback-Leibler Divergence<a class="anchor" aria-label="anchor" href="#kullback-leibler-divergence"></a>
</h3>
<blockquote>
<p><span class="math inline">\(KL(P || Q) = \sum\limits_{i=1}^n P(p_i) * log_2(P(p_i) / P(q_i)) = H(P, Q) - H(P)\)</span></p>
</blockquote>
<p>where <code>H(P, Q)</code> denotes the joint entropy of the probability distributions <code>P</code> and <code>Q</code> and <code>H(P)</code> denotes the entropy of probability distribution <code>P</code>. In case <code>P = Q</code> then <code>KL(P, Q) = 0</code> and in case <code>P != Q</code> then <code>KL(P, Q) &gt; 0</code>.</p>
<p>The KL divergence is a non-symmetric measure of the directed divergence between two probability distributions P and Q. It only fulfills the positivity property of a distance metric.</p>
<p>Because of the relation <code>KL(P||Q) = H(P,Q) - H(P)</code>, the Kullback-Leibler divergence of two probability distributions <code>P</code> and <code>Q</code> is also named <code>Cross Entropy</code> of two probability distributions <code>P</code> and <code>Q</code>.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Kulback-Leibler Divergence between random variables P and Q</span></span>
<span><span class="va">P</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">Q</span> <span class="op">&lt;-</span> <span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">P</span>,<span class="va">Q</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Kulback-Leibler Divergence between P and Q using different log bases</span></span>
<span><span class="fu"><a href="../reference/KL.html">KL</a></span><span class="op">(</span><span class="va">x</span>, unit <span class="op">=</span> <span class="st">"log2"</span><span class="op">)</span> <span class="co"># Default</span></span>
<span><span class="fu"><a href="../reference/KL.html">KL</a></span><span class="op">(</span><span class="va">x</span>, unit <span class="op">=</span> <span class="st">"log"</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/KL.html">KL</a></span><span class="op">(</span><span class="va">x</span>, unit <span class="op">=</span> <span class="st">"log10"</span><span class="op">)</span></span></code></pre></div>
<pre><code># KL(x, unit = "log2") # Default
Kulback-Leibler Divergence using unit 'log2'.
kullback-leibler 
       0.1392629 
# KL(x, unit = "log")
Kulback-Leibler Divergence using unit 'log'.
kullback-leibler 
      0.09652967 
# KL(x, unit = "log10")
Kulback-Leibler Divergence using unit 'log10'.
kullback-leibler 
       0.0419223 </code></pre>
</div>
<div class="section level3">
<h3 id="jensen-shannon-divergence">Jensen-Shannon Divergence<a class="anchor" aria-label="anchor" href="#jensen-shannon-divergence"></a>
</h3>
<p>This function computes the <code>Jensen-Shannon Divergence</code> <code>JSD(P || Q)</code> between two probability distributions <code>P</code> and <code>Q</code> with equal weights <code>π_1</code> = <code>π_2</code> = 1/2.</p>
<p>The Jensen-Shannon Divergence JSD(P || Q) between two probability distributions P and Q is defined as:</p>
<blockquote>
<p><span class="math inline">\(JSD(P || Q) = 0.5 * (KL(P || R) + KL(Q || R))\)</span></p>
</blockquote>
<p>where <code>R = 0.5 * (P + Q)</code> denotes the mid-point of the probability vectors <code>P</code> and <code>Q</code>, and <code>KL(P || R)</code>, <code>KL(Q || R)</code> denote the <code>Kullback-Leibler Divergence</code> of <code>P</code> and <code>R</code>, as well as <code>Q</code> and <code>R</code>.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Jensen-Shannon Divergence between P and Q</span></span>
<span><span class="va">P</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">Q</span> <span class="op">&lt;-</span> <span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">P</span>,<span class="va">Q</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Jensen-Shannon Divergence between P and Q using different log bases</span></span>
<span><span class="fu"><a href="../reference/JSD.html">JSD</a></span><span class="op">(</span><span class="va">x</span>, unit <span class="op">=</span> <span class="st">"log2"</span><span class="op">)</span> <span class="co"># Default</span></span>
<span><span class="fu"><a href="../reference/JSD.html">JSD</a></span><span class="op">(</span><span class="va">x</span>, unit <span class="op">=</span> <span class="st">"log"</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/JSD.html">JSD</a></span><span class="op">(</span><span class="va">x</span>, unit <span class="op">=</span> <span class="st">"log10"</span><span class="op">)</span></span></code></pre></div>
<pre><code># JSD(x, unit = "log2") # Default
Jensen-Shannon Divergence using unit 'log2'.
jensen-shannon 
    0.03792749 
# JSD(x, unit = "log")
Jensen-Shannon Divergence using unit 'log'.
jensen-shannon 
    0.02628933 
# JSD(x, unit = "log10")
Jensen-Shannon Divergence using unit 'log10'.
jensen-shannon 
    0.01141731 </code></pre>
<p>Alternatively, users can specify count data.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Jensen-Shannon Divergence Divergence between count vectors P.count and Q.count</span></span>
<span><span class="va">P.count</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span></span>
<span><span class="va">Q.count</span> <span class="op">&lt;-</span> <span class="fl">20</span><span class="op">:</span><span class="fl">29</span></span>
<span><span class="va">x.count</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">P.count</span>, <span class="va">Q.count</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/JSD.html">JSD</a></span><span class="op">(</span><span class="va">x</span>, est.prob <span class="op">=</span> <span class="st">"empirical"</span><span class="op">)</span></span></code></pre></div>
<pre><code>Jensen-Shannon Divergence using unit 'log2'.
jensen-shannon 
    0.03792749</code></pre>
<p>Or users can compute distances based on a probability matrix</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example: Distance Matrix using JSD-Distance</span></span>
<span><span class="va">Prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>, <span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">)</span>, <span class="fl">30</span><span class="op">:</span><span class="fl">39</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">30</span><span class="op">:</span><span class="fl">39</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># compute the KL matrix of a given probability matrix</span></span>
<span><span class="va">JSDMatrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/JSD.html">JSD</a></span><span class="op">(</span><span class="va">Prob</span><span class="op">)</span></span>
<span></span>
<span><span class="va">JSDMatrix</span></span></code></pre></div>
<pre><code>           v1           v2           v3
v1 0.00000000 0.0379274917 0.0435852218
v2 0.03792749 0.0000000000 0.0002120578
v3 0.04358522 0.0002120578 0.0000000000</code></pre>
<div class="section level4">
<h4 id="properties-of-the-jensen-shannon-divergence">Properties of the <code>Jensen-Shannon Divergence</code>:<a class="anchor" aria-label="anchor" href="#properties-of-the-jensen-shannon-divergence"></a>
</h4>
<ul>
<li><p>JSD is non-negative.</p></li>
<li><p>JSD is a symmetric measure JSD(P || Q) = JSD(Q || P).</p></li>
<li><p>JSD = 0, if and only if P = Q.</p></li>
</ul>
</div>
</div>
<div class="section level3">
<h3 id="generalized-jensen-shannon-divergence">Generalized Jensen-Shannon Divergence<a class="anchor" aria-label="anchor" href="#generalized-jensen-shannon-divergence"></a>
</h3>
<p>The generalized Jensen-Shannon Divergence <span class="math inline">\(gJSD_{\pi_1,...,\pi_n}(P_1, ..., P_n)\)</span> enables distance comparisons between multiple probability distributions <span class="math inline">\(P_1,...,P_n\)</span>:</p>
<blockquote>
<p><span class="math inline">\(gJSD_{\pi_1,...,\pi_n}(P_1, ..., P_n) = H(\sum_{i = 1}^n \pi_i*P_i) - \sum_{i = 1}^n \pi_i*H(P_i)\)</span></p>
</blockquote>
<p>where <span class="math inline">\(\pi_1,...,\pi_n\)</span> denote the weights selected for the probability vectors <span class="math inline">\(P_1,...,P_n\)</span> and <span class="math inline">\(H(P_i)\)</span> denotes the Shannon Entropy of probability vector <span class="math inline">\(P_i\)</span>.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># generate example probability matrix for comparing three probability functions</span></span>
<span><span class="va">Prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>, <span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">)</span>, <span class="fl">30</span><span class="op">:</span><span class="fl">39</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">30</span><span class="op">:</span><span class="fl">39</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># compute the Generalized JSD comparing the PS probability matrix</span></span>
<span><span class="fu"><a href="../reference/gJSD.html">gJSD</a></span><span class="op">(</span><span class="va">Prob</span><span class="op">)</span></span></code></pre></div>
<pre><code>#&gt; No weights were specified ('weights = NULL'), thus equal weights for all
#&gt; distributions will be calculated and applied.
#&gt; Metric: 'gJSD'; unit = 'log2'; comparing: 3 vectors (v1, ... , v3).
#&gt; Weights: v1 = 0.333333333333333, v2 = 0.333333333333333, v3 = 0.333333333333333
[1] 0.03512892</code></pre>
<p>As you can see, the <code>gJSD</code> function prints out the exact number of vectors that were used to compute the generalized JSD. By default, the weights are uniformly distributed (<code>weights = NULL</code>).</p>
<p>Users can also specify non-uniformly distributed weights via specifying the <code>weights</code> argument:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># define probability matrix</span></span>
<span><span class="va">Prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>, <span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">20</span><span class="op">:</span><span class="fl">29</span><span class="op">)</span>, <span class="fl">30</span><span class="op">:</span><span class="fl">39</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fl">30</span><span class="op">:</span><span class="fl">39</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># compute generalized JSD with custom weights</span></span>
<span><span class="fu"><a href="../reference/gJSD.html">gJSD</a></span><span class="op">(</span><span class="va">Prob</span>, weights <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>#&gt; Metric: 'gJSD'; unit = 'log2'; comparing: 3 vectors (v1, ... , v3).
#&gt; Weights: v1 = 0.5, v2 = 0.25, v3 = 0.25
[1] 0.04081969</code></pre>
<p>Finally, users can use the argument <code>est.prob</code> to empirically estimate probability vectors when they wish to specify count vectors as input:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">P.count</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span></span>
<span><span class="va">Q.count</span> <span class="op">&lt;-</span> <span class="fl">20</span><span class="op">:</span><span class="fl">29</span></span>
<span><span class="va">R.count</span> <span class="op">&lt;-</span> <span class="fl">30</span><span class="op">:</span><span class="fl">39</span></span>
<span><span class="va">x.count</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">P.count</span>, <span class="va">Q.count</span>, <span class="va">R.count</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/gJSD.html">gJSD</a></span><span class="op">(</span><span class="va">x.count</span>, est.prob <span class="op">=</span> <span class="st">"empirical"</span><span class="op">)</span></span></code></pre></div>
<pre><code>#&gt; No weights were specified ('weights = NULL'), thus equal weights for all distributions will be calculated and applied.
#&gt; Metric: 'gJSD'; unit = 'log2'; comparing: 3 vectors (v1, ... , v3).
#&gt; Weights: v1 = 0.333333333333333, v2 = 0.333333333333333, v3 = 0.333333333333333
[1] 0.03512892</code></pre>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Hajk-Georg Drost.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
