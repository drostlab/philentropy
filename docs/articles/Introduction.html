<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Introduction • philentropy</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Introduction">
<meta property="og:description" content="philentropy">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">philentropy</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.9.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Distances.html">Distances</a>
    </li>
    <li>
      <a href="../articles/Information_Theory.html">Information Theory</a>
    </li>
    <li>
      <a href="../articles/Introduction.html">Introduction</a>
    </li>
    <li>
      <a href="../articles/Many_Distances.html">Comparing many probability density functions</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/drostlab/philentropy/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Introduction</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/drostlab/philentropy/blob/HEAD/vignettes/Introduction.Rmd" class="external-link"><code>vignettes/Introduction.Rmd</code></a></small>
      <div class="hidden name"><code>Introduction.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction-to-the-philentropy-package">Introduction to the <code>philentropy</code> Package<a class="anchor" aria-label="anchor" href="#introduction-to-the-philentropy-package"></a>
</h2>
<p>Comparison is a fundamental method of scientific research leading to
more general insights about the processes that generate similarity or
dissimilarity. In statistical terms comparisons between probability
functions are performed to infer connections, correlations, or
relationships between samples. The <code>philentropy</code> package
implements optimized distance and similarity measures for comparing
probability functions. These comparisons between probability functions
have their foundations in a broad range of scientific disciplines from
mathematics to ecology. The aim of this package is to provide a base
framework for clustering, classification, statistical inference,
goodness-of-fit, non-parametric statistics, information theory, and
machine learning tasks that are based on comparing univariate or
multivariate probability functions.</p>
<p>Applying the method of comparison in statistics often means computing
distances between probability functions. In this context <a href="https://users.uom.gr/~kouiruki/sung.pdf" class="external-link">Sung-Hyuk Cha (2007)</a>
provides a clear definition of distance :</p>
<blockquote>
<p>From the scientific and mathematical point of view, <em>distance</em>
is defined as a quantitative degree of how far apart two objects
are.</p>
</blockquote>
<p>Hence, quantifying the distance of two objects requires the
assumption about a particular space in which these objects live. For the
euclidean distance, for example, this would mean comparison of objects
(coordinates) in euclidean space (e.g. coordinate system) while other
distance measures may require different spaces to allow sensitive and
appropriate quantification of distances between objects
(e.g. probability space). This aspect of quantifying the
<code>degree of how far two objects are apart in a defined space</code>
(adjusted definition) motivates the existence of diverse distance
measures. <strong>As a result, the domain expert should appreciate the
responsibility to decide in which space their model or experimental data
is best represented and which distance metric then maximizes the
usefulness of object comparison within this defined space</strong>.</p>
<p>Cha’s comprehensive review of distance/similarity measures motivated
me to implement all these measures to better understand their
comparative nature. As Cha states:</p>
<blockquote>
<p>The choice of distance/similarity measures depends on the measurement
type or representation of objects.</p>
</blockquote>
<p>As a result, the <code>philentropy</code> package implements
functions that are part of the following topics:</p>
<ul>
<li>Distance Measure</li>
<li>Information Theory</li>
<li>Correlation Analyses</li>
</ul>
<p>Personally, I hope that some of these functions are helpful to the
scientific community.</p>
<div class="section level3">
<h3 id="distance-measures">Distance Measures<a class="anchor" aria-label="anchor" href="#distance-measures"></a>
</h3>
<p>Here, the <a href="https://github.com/drostlab/philentropy/blob/master/vignettes/Distances.Rmd" class="external-link">Distance
Measure Vignette</a> introduces how to work with the main function
<code><a href="../reference/distance.html">distance()</a></code> that implements the 46 distance measures
presented in Cha’s review.</p>
<p>Furthermore, for each distance/similarity measure a short description
on usage and performance is presented.</p>
<p>The following probability distance/similarity measures will be
described in detail:</p>
</div>
<div class="section level3">
<h3 id="distance-and-similarity-measures">Distance and Similarity Measures<a class="anchor" aria-label="anchor" href="#distance-and-similarity-measures"></a>
</h3>
<div class="section level4">
<h4 id="l_p-minkowski-family">
<span class="math inline">\(L_p\)</span> Minkowski Family<a class="anchor" aria-label="anchor" href="#l_p-minkowski-family"></a>
</h4>
<ul>
<li>Euclidean : <span class="math inline">\(d = \sqrt{\sum_{i = 1}^N |
P_i - Q_i |^2)}\)</span>
</li>
<li>Manhattan : <span class="math inline">\(d = \sum_{i = 1}^N | P_i -
Q_i |\)</span>
</li>
<li>Minkowski : <span class="math inline">\(d = ( \sum_{i = 1}^N | P_i -
Q_i |^p)^{1/p}\)</span>
</li>
<li>Chebyshev : <span class="math inline">\(d = max | P_i - Q_i
|\)</span>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="l_1-family">
<span class="math inline">\(L_1\)</span> Family<a class="anchor" aria-label="anchor" href="#l_1-family"></a>
</h4>
<ul>
<li>Sorensen : <span class="math inline">\(d = \frac{\sum_{i = 1}^N |
P_i - Q_i |}{\sum_{i = 1}^N (P_i + Q_i)}\)</span>
</li>
<li>Gower : <span class="math inline">\(d = \frac{1}{N} \dot \sum_{i =
1}^N | P_i - Q_i |\)</span>, where <span class="math inline">\(N\)</span> is the total number of elements <span class="math inline">\(i\)</span> in <span class="math inline">\(P_i\)</span> and <span class="math inline">\(Q_i\)</span>
</li>
<li>Soergel : <span class="math inline">\(d = \frac{\sum_{i = 1}^N | P_i
- Q_i |}{\sum_{i = 1}^N max(P_i , Q_i)}\)</span>
</li>
<li>Kulczynski d : <span class="math inline">\(d = \frac{\sum_{i = 1}^N
| P_i - Q_i |}{\sum_{i = 1}^N min(P_i , Q_i)}\)</span>
</li>
<li>Canberra : <span class="math inline">\(d = \frac{\sum_{i = 1}^N |
P_i - Q_i |}{(P_i + Q_i)}\)</span>
</li>
<li>Lorentzian : <span class="math inline">\(d = \sum_{i = 1}^N ln(1 + |
P_i - Q_i |)\)</span>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="intersection-family">Intersection Family<a class="anchor" aria-label="anchor" href="#intersection-family"></a>
</h4>
<ul>
<li>Intersection : <span class="math inline">\(s = \sum_{i = 1}^N
min(P_i , Q_i)\)</span>
</li>
<li>Non-Intersection : <span class="math inline">\(d = 1 - \sum_{i =
1}^N min(P_i , Q_i)\)</span>
</li>
<li>Wave Hedges : <span class="math inline">\(d = \frac{\sum_{i = 1}^N |
P_i - Q_i |}{max(P_i , Q_i)}\)</span>
</li>
<li>Czekanowski : <span class="math inline">\(d = \frac{\sum_{i = 1}^N |
P_i - Q_i |}{\sum_{i = 1}^N | P_i + Q_i |}\)</span>
</li>
<li>Motyka : <span class="math inline">\(d = \frac{\sum_{i = 1}^N
min(P_i , Q_i)}{(P_i + Q_i)}\)</span>
</li>
<li>Kulczynski s : <span class="math inline">\(d = \frac{\sum_{i = 1}^N
min(P_i , Q_i)}{\sum_{i = 1}^N | P_i - Q_i |}\)</span>
</li>
<li>Tanimoto : <span class="math inline">\(d = \frac{\sum_{i = 1}^N
(max(P_i , Q_i) - min(P_i , Q_i))}{\sum_{i = 1}^N max(P_i ,
Q_i)}\)</span> ; equivalent to Soergel</li>
<li>Ruzicka : <span class="math inline">\(s = \frac{\sum_{i = 1}^N
min(P_i , Q_i)}{\sum_{i = 1}^N max(P_i , Q_i)}\)</span> ; equivalent to
1 - Tanimoto = 1 - Soergel</li>
</ul>
</div>
<div class="section level4">
<h4 id="inner-product-family">Inner Product Family<a class="anchor" aria-label="anchor" href="#inner-product-family"></a>
</h4>
<ul>
<li>Inner Product : <span class="math inline">\(s = \sum_{i = 1}^N P_i
\dot Q_i\)</span>
</li>
<li>Harmonic mean : <span class="math inline">\(s = 2 \cdot \frac{
\sum_{i = 1}^N P_i \cdot Q_i}{P_i + Q_i}\)</span>
</li>
<li>Cosine : <span class="math inline">\(s = \frac{\sum_{i = 1}^N P_i
\cdot Q_i}{\sqrt{\sum_{i = 1}^N P_i^2} \cdot \sqrt{\sum_{i = 1}^N
Q_i^2}}\)</span>
</li>
<li>Kumar-Hassebrook (PCE) : <span class="math inline">\(s =
\frac{\sum_{i = 1}^N (P_i \cdot Q_i)}{(\sum_{i = 1}^N P_i^2 + \sum_{i =
1}^N Q_i^2 - \sum_{i = 1}^N (P_i \cdot Q_i))}\)</span>
</li>
<li>Jaccard : <span class="math inline">\(d = 1 - \frac{\sum_{i = 1}^N
P_i \cdot Q_i}{\sum_{i = 1}^N P_i^2 + \sum_{i = 1}^N Q_i^2 - \sum_{i =
1}^N P_i \cdot Q_i}\)</span> ; equivalent to 1 - Kumar-Hassebrook</li>
<li>Dice : <span class="math inline">\(d = \frac{\sum_{i = 1}^N (P_i -
Q_i)^2}{(\sum_{i = 1}^N P_i^2 + \sum_{i = 1}^N Q_i^2)}\)</span>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="squared-chord-family">Squared-chord Family<a class="anchor" aria-label="anchor" href="#squared-chord-family"></a>
</h4>
<ul>
<li>Fidelity : <span class="math inline">\(s = \sum_{i = 1}^N \sqrt{P_i
\cdot Q_i}\)</span>
</li>
<li>Bhattacharyya : <span class="math inline">\(d = - ln \sum_{i = 1}^N
\sqrt{P_i \cdot Q_i}\)</span>
</li>
<li>Hellinger : <span class="math inline">\(d = 2 \cdot \sqrt{1 -
\sum_{i = 1}^N \sqrt{P_i \cdot Q_i}}\)</span>
</li>
<li>Matusita : <span class="math inline">\(d = \sqrt{2 - 2 \cdot \sum_{i
= 1}^N \sqrt{P_i \cdot Q_i}}\)</span>
</li>
<li>Squared-chord : <span class="math inline">\(d = \sum_{i = 1}^N (
\sqrt{P_i} - \sqrt{Q_i} )^2\)</span>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="squared-l_2-family-x2-squared-family">Squared <span class="math inline">\(L_2\)</span> family (<span class="math inline">\(X^2\)</span> squared family)<a class="anchor" aria-label="anchor" href="#squared-l_2-family-x2-squared-family"></a>
</h4>
<ul>
<li>Squared Euclidean : <span class="math inline">\(d = \sum_{i = 1}^N (
P_i - Q_i )^2\)</span>
</li>
<li>Pearson <span class="math inline">\(X^2\)</span> : <span class="math inline">\(d = \sum_{i = 1}^N ( \frac{(P_i - Q_i )^2}{Q_i}
)\)</span>
</li>
<li>Neyman <span class="math inline">\(X^2\)</span> : <span class="math inline">\(d = \sum_{i = 1}^N ( \frac{(P_i - Q_i )^2}{P_i}
)\)</span>
</li>
<li>Squared <span class="math inline">\(X^2\)</span> : <span class="math inline">\(d = \sum_{i = 1}^N ( \frac{(P_i - Q_i )^2}{(P_i +
Q_i)} )\)</span>
</li>
<li>Probabilistic Symmetric <span class="math inline">\(X^2\)</span> :
<span class="math inline">\(d = 2 \cdot \sum_{i = 1}^N ( \frac{(P_i -
Q_i )^2}{(P_i + Q_i)} )\)</span>
</li>
<li>Divergence : <span class="math inline">\(X^2\)</span> : <span class="math inline">\(d = 2 \cdot \sum_{i = 1}^N ( \frac{(P_i - Q_i
)^2}{(P_i + Q_i)^2} )\)</span>
</li>
<li>Clark : <span class="math inline">\(d = \sqrt{\sum_{i = 1}^N
(\frac{| P_i - Q_i |}{(P_i + Q_i)^2}}\)</span>
</li>
<li>Additive Symmetric <span class="math inline">\(X^2\)</span> : <span class="math inline">\(d = \sum_{i = 1}^N ( \frac{((P_i - Q_i)^2 \cdot
(P_i + Q_i))}{(P_i \cdot Q_i)} )\)</span>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="shannons-entropy-family">Shannon’s Entropy Family<a class="anchor" aria-label="anchor" href="#shannons-entropy-family"></a>
</h4>
<ul>
<li>Kullback-Leibler : <span class="math inline">\(d = \sum_{i = 1}^N
P_i \cdot log(\frac{P_i}{Q_i})\)</span>
</li>
<li>Jeffreys : <span class="math inline">\(d = \sum_{i = 1}^N (P_i -
Q_i) \cdot log(\frac{P_i}{Q_i})\)</span>
</li>
<li>K divergence : <span class="math inline">\(d = \sum_{i = 1}^N P_i
\cdot log(\frac{2 \cdot P_i}{P_i + Q_i})\)</span>
</li>
<li>Topsoe : <span class="math inline">\(d = \sum_{i = 1}^N ( P_i \cdot
log(\frac{2 \cdot P_i}{P_i + Q_i}) ) + ( Q_i \cdot log(\frac{2 \cdot
Q_i}{P_i + Q_i}) )\)</span>
</li>
<li>Jensen-Shannon : <span class="math inline">\(d = 0.5 \cdot ( \sum_{i
= 1}^N P_i \cdot log(\frac{2 \cdot P_i}{P_i + Q_i}) + \sum_{i = 1}^N Q_i
\cdot log(\frac{2 * Q_i}{P_i + Q_i}))\)</span>
</li>
<li>Jensen difference : <span class="math inline">\(d = \sum_{i = 1}^N (
(\frac{P_i \cdot log(P_i) + Q_i \cdot log(Q_i)}{2}) - (\frac{P_i +
Q_i}{2}) \cdot log(\frac{P_i + Q_i}{2}) )\)</span>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="combinations">Combinations<a class="anchor" aria-label="anchor" href="#combinations"></a>
</h4>
<ul>
<li>Taneja : <span class="math inline">\(d = \sum_{i = 1}^N ( \frac{P_i
+ Q_i}{2}) \cdot log( \frac{P_i + Q_i}{( 2 \cdot \sqrt{P_i \cdot Q_i})}
)\)</span>
</li>
<li>Kumar-Johnson : <span class="math inline">\(d = \sum_{i = 1}^N
\frac{(P_i^2 - Q_i^2)^2}{2 \cdot (P_i \cdot
Q_i)^{\frac{3}{2}}}\)</span>
</li>
<li>Avg(<span class="math inline">\(L_1\)</span>, <span class="math inline">\(L_n\)</span>) : <span class="math inline">\(d =
\frac{\sum_{i = 1}^N | P_i - Q_i| + max{ | P_i - Q_i
|}}{2}\)</span>
</li>
</ul>
<p><strong>Note</strong>: <span class="math inline">\(d\)</span> refers
to distance measures, whereas <span class="math inline">\(s\)</span>
denotes similarity measures.</p>
</div>
</div>
<div class="section level3">
<h3 id="information-theory">Information Theory<a class="anchor" aria-label="anchor" href="#information-theory"></a>
</h3>
<p>Modern methods for distribution comparisons have a strong <a href="http://compbio.biosci.uq.edu.au/mediawiki/upload/b/b3/Jaynes_PhysRev1957-1.pdf" class="external-link">information
theoretic background</a>. This fact motivated me to name this package
<code>philentropy</code> and as a result, several well established
information theory measures are (and further will be) implemented in
this package.</p>
<ul>
<li>Shannon’s Entropy H(X) : <span class="math inline">\(H(X) =
-\sum\limits_{i=1}^n P(x_i) \cdot log_b(P(x_i))\)</span>
</li>
<li>Shannon’s Joint-Entropy H(X,Y) : <span class="math inline">\(H(X,Y)
= -\sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) \cdot log_b(P(x_i,
y_j))\)</span>
</li>
<li>Shannon’s Conditional-Entropy H(X | Y) : <span class="math inline">\(H(Y|X) = \sum\limits_{i=1}^n\sum\limits_{j=1}^m
P(x_i, y_j) \cdot log_b( \frac{P(x_i)}{P(x_i, y_j)})\)</span>
</li>
<li>Mutual Information I(X,Y) : <span class="math inline">\(MI(X,Y) =
\sum\limits_{i=1}^n\sum\limits_{j=1}^m P(x_i, y_j) \cdot log_b(
\frac{P(x_i, y_j)}{( P(x_i) * P(y_j) )})\)</span>
</li>
<li>Kullback-Leibler Divergence : <span class="math inline">\(KL(P || Q)
= \sum\limits_{i=1}^n P(p_i) \cdot log_2(\frac{P(p_i) }{P(q_i)}) = H(P,
Q) - H(P)\)</span>
</li>
<li>Jensen-Shannon Divergence : <span class="math inline">\(JSD(P || Q)
= 0.5 * (KL(P || R) + KL(Q || R))\)</span>
</li>
<li>Generalized Jensen-Shannon Divergence : <span class="math inline">\(gJSD_{\pi_1,...,\pi_n}(P_1, ..., P_n) = H(\sum_{i
= 1}^n \pi_i \cdot P_i) - \sum_{i = 1}^n \pi_i \cdot
H(P_i)\)</span>
</li>
</ul>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Hajk-Georg Drost.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
